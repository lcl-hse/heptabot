{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Model training.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"YONnGjpAYUdU","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/isikus/qualification-project/blob/master/notebooks/3.%20Model%20training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"zrtR2urJV3ST","colab_type":"text"},"source":["##### Copyright 2020 The T5 Authors\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");"]},{"cell_type":"code","metadata":{"id":"DWdCSqJ6WHBh","colab_type":"code","colab":{}},"source":["# Copyright 2019 The T5 Authors. All Rights Reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","# =============================================================================="],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FnST7D2sVBd1","colab_type":"text"},"source":["# Model training\n","In this notebook we retrain the most successful model from our research to the same checkpoint. This notebook is based on [this](github/google-research/text-to-text-transfer-transformer/blob/master/notebooks/t5-trivia.ipynb) example notebook from T5 authors.\n","\n","**Please note the following:**\n","1. A connection to a Google Cloud Storage bucket is required to train the model."]},{"cell_type":"markdown","metadata":{"id":"yAb_APDrefs6","colab_type":"text"},"source":["## Set Up"]},{"cell_type":"markdown","metadata":{"id":"eDeE_yVuHMYg","colab_type":"text"},"source":["<h3><a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>  &nbsp;&nbsp;Train on TPU</h3>\n","\n","\n","\n","\n","   1. Create a Cloud Storage bucket for your data and model checkpoints at http://console.cloud.google.com/storage, and fill in the `BASE_DIR` parameter in the following form. There is a [free tier](https://cloud.google.com/free/) if you do not yet have an account.\n"," \n","   1. On the main menu, click Runtime and select **Change runtime type**. Set \"TPU\" as the hardware accelerator.\n","   1. Run the following cell and follow instructions to:\n","    *  Set up a Colab TPU running environment\n","    *   Verify that you are connected to a TPU device\n","    *   Upload your credentials to TPU to access your GCS bucket\n"]},{"cell_type":"code","metadata":{"id":"o68EireWQAp8","colab_type":"code","colab":{}},"source":["print(\"Installing dependencies...\")\n","%tensorflow_version 2.x\n","!pip install -q t5\n","\n","import functools\n","import os\n","import time\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","import tensorflow.compat.v1 as tf\n","import tensorflow_datasets as tfds\n","\n","import t5\n","\n","BASE_DIR = \"gs://\" #@param { type: \"string\" }\n","if not BASE_DIR or BASE_DIR == \"gs://\":\n","  raise ValueError(\"You must enter a BASE_DIR.\")\n","DATA_DIR = os.path.join(BASE_DIR, \"data\")\n","MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n","ON_CLOUD = True\n","\n","\n","if ON_CLOUD:\n","  print(\"Setting up GCS access...\")\n","  import tensorflow_gcs_config\n","  from google.colab import auth\n","  # Set credentials for GCS reading/writing from Colab and TPU.\n","  TPU_TOPOLOGY = \"2x2\"\n","  try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","    TPU_ADDRESS = tpu.get_master()\n","    print('Running on TPU:', TPU_ADDRESS)\n","  except ValueError:\n","    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n","  auth.authenticate_user()\n","  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n","  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n","\n","tf.disable_v2_behavior()\n","\n","# Improve logging.\n","from contextlib import contextmanager\n","import logging as py_logging\n","\n","if ON_CLOUD:\n","  tf.get_logger().propagate = False\n","  py_logging.root.setLevel('INFO')\n","\n","@contextmanager\n","def tf_verbosity_level(level):\n","  og_level = tf.logging.get_verbosity()\n","  tf.logging.set_verbosity(level)\n","  yield\n","  tf.logging.set_verbosity(og_level)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5XH9lh-gQAp_","colab_type":"code","colab":{}},"source":["from google.colab import auth\n","auth.authenticate_user()\n","project_id = 'project-id'  # @param {\"type\": \"string\"}\n","\n","!gcloud config set project {project_id}\n","!gsutil ls"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3vVHx6AVGrtM","colab_type":"text"},"source":["### Try to add reproducibility"]},{"cell_type":"code","metadata":{"id":"M3c-wp6n_8O1","colab_type":"code","colab":{}},"source":["import random\n","import numpy as np\n","\n","def set_seed(seed):\n","  random.seed(seed)\n","  np.random.seed(seed)\n","  tf.compat.v1.set_random_seed(seed)\n","\n","set_seed(42)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dMoJ-G9mqDqa","colab_type":"text"},"source":["## Create new Tasks and Mixture"]},{"cell_type":"markdown","metadata":{"id":"zwoLPQhE6bef","colab_type":"text"},"source":["Two core components of the T5 library are `Task` and `Mixture` objects.\n","\n","A `Task` is a dataset along with preprocessing functions and evaluation metrics. A `Mixture` is a collection of `Task` objects along with a mixing rate or a function defining how to compute a mixing rate based on the properties of the constituent `Tasks`.\n","\n","We will describe one `Task` which is to correct our examples and then sample the `Mixture` from this only task.\n","\n","Later on, we will repeat the process to continue model training."]},{"cell_type":"code","metadata":{"id":"OjEonhK3zNRu","colab_type":"code","colab":{}},"source":["import gzip\n","import json\n","\n","tsv_path = {\n","    \"train\": os.path.join(DATA_DIR, \"3b-fuse1.tsv\"),\n","    \"validation\": os.path.join(DATA_DIR, \"correct-val.tsv\")\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R-Ja8akCX1dR","colab_type":"text"},"source":["Here a function is defined to load the TSV data as a `tf.data.Dataset` in TensorFlow."]},{"cell_type":"code","metadata":{"id":"KPOteeqctpzw","colab_type":"code","colab":{}},"source":["def corr_dataset_fn(split, shuffle_files=False):\n","  # We only have one file for each split.\n","  del shuffle_files\n","\n","  # Load lines from the text file as examples.\n","  ds = tf.data.TextLineDataset(tsv_path[split])\n","  # Split each \"<orig_text>\\t<corr_text>\" example into (orig_text, corr_text) tuple.\n","  ds = ds.map(\n","      functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n","                        field_delim=\"\\t\", use_quote_delim=False),\n","      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","  # Map each tuple to a {\"orig_text\": ... \"corr_text\": ...} dict.\n","  ds = ds.map(lambda *ex: dict(zip([\"orig_text\", \"corr_text\"], ex)))\n","  return ds\n","\n","print(\"A few raw validation examples...\")\n","for ex in tfds.as_numpy(corr_dataset_fn(\"validation\").take(5)):\n","  print(ex)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MCUYT7JmX9Tj","colab_type":"text"},"source":["Now, we write a preprocess function to convert the examples in the `tf.data.Dataset` into a text-to-text format, with both `inputs` and `targets` fields. The preprocessor also normalizes the text by lowercasing it and removing quotes since the corr_texts are sometimes formatted in odd ways. Finally, we prepend 'correct: ' to the inputs so that the model knows what task it is trying to solve."]},{"cell_type":"code","metadata":{"id":"x8tNn6HMYLMb","colab_type":"code","colab":{}},"source":["def correction_preprocessor(ds):\n","  def normalize_text(text):\n","    \"\"\"Remove quotes from a TensorFlow string.\"\"\"\n","    text = tf.strings.regex_replace(text,\"'(.*)'\", r\"\\1\")\n","    return text\n","\n","  def to_inputs_and_targets(ex):\n","    \"\"\"Map {\"orig_text\": ..., \"corr_text\": ...}->{\"inputs\": ..., \"targets\": ...}.\"\"\"\n","    return {\n","        \"inputs\":\n","             tf.strings.join(\n","                 [\"correction: \", normalize_text(ex[\"orig_text\"])]),\n","        \"targets\": normalize_text(ex[\"corr_text\"])\n","    }\n","  return ds.map(to_inputs_and_targets, \n","                num_parallel_calls=tf.data.experimental.AUTOTUNE)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gm1Pm2aRZ9Ow","colab_type":"text"},"source":["Finally, we put everything together to create a `Task`."]},{"cell_type":"code","metadata":{"id":"yJyRavOpZ7UW","colab_type":"code","colab":{}},"source":["t5.data.TaskRegistry.add(\n","    \"correct_3b\",\n","    # Supply a function which returns a tf.data.Dataset.\n","    dataset_fn=corr_dataset_fn,\n","    splits=[\"train\", \"validation\"],\n","    # Supply a function which preprocesses text from the tf.data.Dataset.\n","    text_preprocessor=[correction_preprocessor],\n","    # Use the same vocabulary that we used for pre-training.\n","    sentencepiece_model_path=t5.data.DEFAULT_SPM_PATH,\n","    # Lowercase targets before computing metrics.\n","    postprocess_fn=t5.data.postprocessors.lower_text, \n","    # We'll use accuracy as our evaluation metric.\n","    metric_fns=[t5.evaluation.metrics.accuracy,\n","                t5.evaluation.metrics.bleu,\n","                t5.evaluation.metrics.rouge]\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qe4o_0jFbP-p","colab_type":"text"},"source":["Let's look at a few pre-processed examples from the validation set."]},{"cell_type":"code","metadata":{"id":"I64TqHGxbOJ2","colab_type":"code","colab":{}},"source":["corr_task = t5.data.TaskRegistry.get(\"correct_3b\")\n","ds = corr_task.get_dataset(split=\"validation\", sequence_length={\"inputs\": 512, \"targets\": 512})\n","print(\"A few preprocessed validation examples...\")\n","for ex in tfds.as_numpy(ds.take(5)):\n","  print(ex)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wlghm_3rAd-M","colab_type":"text"},"source":["## Dataset Mixture\n","\n","We now create a `Mixture` from the above `Task`, which we will fine-tune on."]},{"cell_type":"code","metadata":{"id":"Zgs-s3eDAU37","colab_type":"code","colab":{}},"source":["t5.data.MixtureRegistry.remove(\"correct_3b_all\")\n","t5.data.MixtureRegistry.add(\n","    \"correct_3b_all\",\n","    [\"correct_3b\"],\n","     default_rate=1.0\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CUkorodCENGw","colab_type":"text"},"source":["## Transferring to new Tasks\n","\n","We are now ready to fine-tune one of the pre-trained T5 models on our new mixture.\n","\n","First, we'll instantiate a `Model`.\n","\n","\n","## Caveats\n","\n","* Due to its memory requirements, you will not be able to train the `11B` parameter model on the TPU provided by Colab. Instead, you will need to fine-tune inside of a GCP instance (see [README](https://github.com/google-research/text-to-text-transfer-transformer/)).\n","* Due to the checkpoint size, you will not be able use the 5GB GCS free tier for the `3B` parameter models. You will need at least 25GB of space, which you can purchase with your $300 of initial credit on GCP.\n","* While `large` can achieve decent results, it is recommended that you fine-tune at least the `3B` parameter model.\n"]},{"cell_type":"markdown","metadata":{"id":"syte5n0nnMOC","colab_type":"text"},"source":["## Define Model"]},{"cell_type":"code","metadata":{"id":"pesK1uu6QAqx","colab_type":"code","colab":{}},"source":["run = \"3b-retrain\"  # @param {\"type\": \"string\"}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"both","id":"yGQ-zpgy3raf","colab_type":"code","colab":{}},"source":["MODEL_SIZE = \"3B\" #@param[\"small\", \"base\", \"large\", \"3B\", \"11B\"]\n","# Public GCS path for T5 pre-trained model checkpoints\n","BASE_PRETRAINED_DIR = \"gs://t5-data/pretrained_models\"\n","PRETRAINED_DIR = os.path.join(BASE_PRETRAINED_DIR, MODEL_SIZE)\n","if run not in [None, \"\"]:\n","    MODEL_DIR = os.path.join(MODELS_DIR, MODEL_SIZE+\"-\"+run)\n","else:\n","    MODEL_DIR = os.path.join(MODELS_DIR, MODEL_SIZE)\n","\n","if ON_CLOUD and MODEL_SIZE == \"3B\":\n","  tf.logging.warn(\n","      \"The `3B` model is too large to use with the 5GB GCS free tier. \"\n","      \"Make sure you have at least 25GB on GCS before continuing.\"\n","  )\n","elif ON_CLOUD and MODEL_SIZE == \"11B\":\n","  raise ValueError(\n","      \"The `11B` parameter is too large to fine-tune on the `v2-8` TPU \"\n","      \"provided by Colab. Please comment out this Error if you're running \"\n","      \"on a larger TPU.\"\n","  )\n","\n","# Set parallelism and batch size to fit on v2-8 TPU (if possible).\n","# Limit number of checkpoints to fit within 5GB (if possible).\n","model_parallelism, train_batch_size, keep_checkpoint_max = {\n","    \"small\": (1, 128, 16),\n","    \"base\": (2, 64, 8),\n","    \"large\": (8, 32, 4),\n","    \"3B\": (8, 8, 1),\n","    \"11B\": (8, 8, 1)}[MODEL_SIZE]\n","\n","tf.io.gfile.makedirs(MODEL_DIR)\n","# The models from our paper are based on the Mesh Tensorflow Transformer.\n","model = t5.models.MtfModel(\n","    model_dir=MODEL_DIR,\n","    tpu=TPU_ADDRESS,\n","    tpu_topology=TPU_TOPOLOGY,\n","    model_parallelism=model_parallelism,\n","    batch_size=train_batch_size,\n","    sequence_length={\"inputs\": 512, \"targets\": 512},\n","    learning_rate_schedule=0.0025,\n","    save_checkpoints_steps=5000,\n","    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n","    iterations_per_loop=100,\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DZhAd0U_4B_o","colab_type":"text"},"source":["## Fine-tune\n","\n","We are now ready to fine-tune our model. This will take a while, so please be patient!\n","\n","Don't worry, you can always come back later and increase the number of steps, and it will automatically pick up where you left off."]},{"cell_type":"code","metadata":{"id":"V7t7a25LBTj9","colab_type":"code","colab":{}},"source":["FINETUNE_STEPS = 15000\n","print(\"Finetuning for\", FINETUNE_STEPS, \"steps\")\n","\n","model.finetune(\n","    mixture_or_task_name=\"correct_3b_all\",\n","    pretrained_model_dir=PRETRAINED_DIR,\n","    finetune_steps=FINETUNE_STEPS\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zJQO_QbdK3gS"},"source":["## Repeat the procedure"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2jShfysHK3gW"},"source":["In order to fuse the same, but now spell-checked, data into the model, we redo the previous procedures with the new data source:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"q7T0myzKK3gX","colab":{}},"source":["tsv_path = {\n","    \"train\": os.path.join(DATA_DIR, \"3b-fuse2.tsv\"),\n","    \"validation\": os.path.join(DATA_DIR, \"correct-val.tsv\")\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"H3470t85K3go","colab":{}},"source":["t5.data.TaskRegistry.add(\n","    \"correct_3b\",\n","    # Supply a function which returns a tf.data.Dataset.\n","    dataset_fn=corr_dataset_fn,\n","    splits=[\"train\", \"validation\"],\n","    # Supply a function which preprocesses text from the tf.data.Dataset.\n","    text_preprocessor=[correction_preprocessor],\n","    # Use the same vocabulary that we used for pre-training.\n","    sentencepiece_model_path=t5.data.DEFAULT_SPM_PATH,\n","    # Lowercase targets before computing metrics.\n","    postprocess_fn=t5.data.postprocessors.lower_text, \n","    # We'll use accuracy as our evaluation metric.\n","    metric_fns=[t5.evaluation.metrics.accuracy,\n","                t5.evaluation.metrics.bleu,\n","                t5.evaluation.metrics.rouge]\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tmWTtlywK3gt","colab":{}},"source":["corr_task = t5.data.TaskRegistry.get(\"correct_3b\")\n","ds = corr_task.get_dataset(split=\"validation\", sequence_length={\"inputs\": 512, \"targets\": 512})\n","print(\"A few preprocessed validation examples...\")\n","for ex in tfds.as_numpy(ds.take(5)):\n","  print(ex)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cRs28l0vK3gx","colab":{}},"source":["t5.data.MixtureRegistry.remove(\"correct_3b_all\")\n","t5.data.MixtureRegistry.add(\n","    \"correct_3b_all\",\n","    [\"correct_3b\"],\n","     default_rate=1.0\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GFHBy8iAK3hH"},"source":["## Fine-tune\n","\n","And now we continue fine-tuning up to step 25200."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MSIVtU26K3hI","colab":{}},"source":["FINETUNE_STEPS = 25200\n","print(\"Finetuning for\", FINETUNE_STEPS, \"steps\")\n","\n","model.finetune(\n","    mixture_or_task_name=\"correct_3b_all\",\n","    pretrained_model_dir=PRETRAINED_DIR,\n","    finetune_steps=FINETUNE_STEPS\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eYeciUZ_D7T2","colab_type":"text"},"source":["## Evaluate\n","\n","We now evaluate on the validation sets of the tasks in our mixture."]},{"cell_type":"code","metadata":{"id":"bz6CJRHzNfd3","colab_type":"code","colab":{}},"source":["%%time\n","\n","# Use a larger batch size for evaluation, which requires less memory.\n","model.batch_size = train_batch_size * 4\n","model.eval(\n","    mixture_or_task_name=\"correct_3b_all\",\n","    checkpoint_steps=-1  # use latest checkpoint\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"92dClA1SWwIx","colab_type":"text"},"source":["Let's look at a few random predictions from the validation sets. Note that we measure accuracy based on an *exact match* of the predicted correction and the ground-truth correction. As a result, all the correction will be likely counted as false."]},{"cell_type":"code","metadata":{"id":"-FuqHRuvxOct","colab_type":"code","colab":{}},"source":["import random\n","\n","def print_random_predictions(task_name, n=10):\n","  \"\"\"Print n predictions from the validation split of a task.\"\"\"\n","  # Grab the dataset for this task.\n","  ds = t5.data.TaskRegistry.get(task_name).get_dataset(\n","      split=\"validation\",\n","      sequence_length={\"inputs\": 512, \"targets\": 512},\n","      shuffle=False)\n","\n","  def _prediction_file_to_ckpt(path):\n","    \"\"\"Extract the global step from a prediction filename.\"\"\"\n","    return int(path.split(\"_\")[-2])\n","\n","  # Grab the paths of all logged predictions.\n","  prediction_files = tf.io.gfile.glob(\n","      os.path.join(\n","          MODEL_DIR,\n","          \"validation_eval/%s_*_predictions\" % task_name))\n","  # Get most recent prediction file by sorting by their step.\n","  latest_prediction_file = sorted(\n","      prediction_files, key=_prediction_file_to_ckpt)[-1]\n","\n","  # Collect (inputs, targets, prediction) from the dataset and predictions file\n","  results = []\n","  with tf.io.gfile.GFile(latest_prediction_file) as preds:\n","    for ex, pred in zip(tfds.as_numpy(ds), preds):\n","      results.append((tf.compat.as_text(ex[\"inputs_plaintext\"]),\n","                      tf.compat.as_text(ex[\"targets_plaintext\"]),\n","                      pred.strip()))\n","\n","  print(\"<== Random predictions for %s using checkpoint %s ==>\\n\" %\n","        (task_name, \n","         _prediction_file_to_ckpt(latest_prediction_file)))\n","\n","  for inp, tgt, pred in random.choices(results, k=10):\n","    print(\"Input:\", inp)\n","    print(\"Target:\", tgt)\n","    print(\"Prediction:\", pred)\n","    print(\"Counted as Correct?\", tgt == pred)\n","    print()\n","\n","print_random_predictions(\"correct_3b\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vbqiq2Ab4PJk","colab_type":"text"},"source":["## Predict\n","\n","Now that we have fine-tuned the model, we can feed T5 arbitrary texts and have it correct them for us!\n","\n","There is a significant amount of overhead in initializing the model so this may take a few minutes to run each time even though the prediction itself is quite fast.\n"]},{"cell_type":"code","metadata":{"cellView":"both","id":"xatHPuCJsPns","colab_type":"code","colab":{}},"source":["orig_text_1 = \"In 1999 theunemployment figures has had a decrease on 15 percents.\" #@param {type:\"string\"}\n","orig_text_2 = \"I agree with the statement because part-time job gave me lots of experiences. For example, I understood following things through part-time jobs. It is hard to make money because I have to go to part-time job when I promised day and time, even if I don't want it. I have to keep time because if I delayed, I interrupted colleague's jobs and make bad relationships. I can't stay I'm center of the world because guests choice restaurant which they want to go, so I need to make them comfortable for they choose my shop. I learned from part-time jobs how important money and to make money, to make relationship. But I learned it is makes me tired to have job at the same time. I need time to have a rest for recovery. So it is interrupt my study to have too much jobs. I agree with having a part-time job but it is better to work during long vacation like summer or spring. Because student first priority is study and part-time job should not interrupt their study.\" #@param {type:\"string\"}\n","orig_text_3 = \"Internet is a new Fenomenon in our world. Our generation is aboped to internet easy, but How previous generation react introduse in our live. \\u003Cbr> On diagram is showed persent of online adult in USA who use networks. What can we see? Firstly, the people's activity over 65 year old is lower than activity of another groups. Next, we can see that then higher age than lower share of people who use comunicatin networks (such as Facebook or instagram). But it is not true for work networks (such as Linkedin). Share People with age between 40 and 64 use work networks higher then share of othor groups. About Facebook. It is most popular network in all groups. It may be explained by different funccioning of this network.\" #@param {type:\"string\"}\n","orig_text_4 = \"I always had consider myself been very looky to be born in the  Century because  all the amaizing human inventions. \\u003Cbr> As an example let's have a look and we will see of this nation  how it has developed. We just need to think about the first world wold and all the inventions created thanks to the industrial revolution to these day. \\u003Cbr> However I feel that at the moment there should emerge more grups to control this development because  the industrial polution that it has been created and it seems that its very little what has been done at the moment in relation to our embiroment. \\u003Cbr> I hope in the future there will be more and more people interested in this important issue.\" #@param {type:\"string\"}\n","\n","orig_texts = [orig_text_1, orig_text_2, orig_text_3, orig_text_4]\n","#orig_texts = [correct(text) for text in orig_texts]\n","\n","now = time.time()\n","# Write out the supplied orig_texts to text files.\n","predict_inputs_path = os.path.join(MODEL_DIR, \"predict_inputs_%d.txt\" % now)\n","predict_outputs_path = os.path.join(MODEL_DIR, \"predict_outputs_%d.txt\" % now)\n","# Manually apply preprocessing by prepending \"triviaqa orig_text:\".\n","with tf.io.gfile.GFile(predict_inputs_path, \"w\") as f:\n","  for q in orig_texts:\n","    f.write(\"correction: %s\\n\" % q.lower())\n","\n","# Ignore any logging so that we only see the model's corr_texts to the orig_texts.\n","with tf_verbosity_level('ERROR'):\n","  model.batch_size = 8  # Min size for small model on v2-8 with parallelism 1.\n","  model.predict(\n","      input_file=predict_inputs_path,\n","      output_file=predict_outputs_path,\n","      # Select the most probable output token at each step.\n","      temperature=0,\n","  )\n","\n","# The output filename will have the checkpoint appended so we glob to get \n","# the latest.\n","prediction_files = sorted(tf.io.gfile.glob(predict_outputs_path + \"*\"))\n","print(\"\\nPredictions using checkpoint %s:\\n\" % prediction_files[-1].split(\"-\")[-1])\n","with tf.io.gfile.GFile(prediction_files[-1]) as f:\n","  for q, a in zip(orig_texts, f):\n","    if q:\n","      print(\"Q: \" + q)\n","      print(\"A: \" + a)\n","      print()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l_YuEL9FZ-UR","colab_type":"text"},"source":["## Export SavedModel\n","\n","Finally, here one can save and load a model."]},{"cell_type":"code","metadata":{"id":"eWu8lbh3aHjc","colab_type":"code","colab":{}},"source":["%%time\n","\n","export_dir = os.path.join(MODEL_DIR, \"export\")\n","\n","model.batch_size = 1 # make one prediction per call\n","saved_model_path = model.export(\n","    export_dir,\n","    checkpoint_step=-1,  # use most recent\n","    beam_size=1,  # no beam search\n","    temperature=1.0,  # sample according to predicted distribution\n",")\n","print(\"Model saved to:\", saved_model_path)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PZ8WXpFkaNTP","colab_type":"text"},"source":["## Load SavedModel"]},{"cell_type":"code","metadata":{"id":"1TpeMFGhaN7r","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import tensorflow_text  # Required to run exported model.\n","\n","def load_predict_fn(model_path):\n","  if tf.executing_eagerly() and False:  # eager execution somehow not working\n","    print(\"Loading SavedModel in eager mode.\")\n","    imported = tf.saved_model.load(model_path, [\"serve\"])\n","    return lambda x: imported.signatures['serving_default'](tf.constant(x))['outputs'].numpy()\n","  else:\n","    print(\"Loading SavedModel in tf 1.x graph mode.\")\n","    with tf.device('/cpu:0'):\n","      tf.compat.v1.reset_default_graph()\n","      sess = tf.compat.v1.Session()\n","      meta_graph_def = tf.compat.v1.saved_model.load(sess, [\"serve\"], model_path)\n","      signature_def = meta_graph_def.signature_def[\"serving_default\"]\n","      return lambda x: sess.run(\n","          fetches=signature_def.outputs[\"outputs\"].name, \n","          feed_dict={signature_def.inputs[\"input\"].name: x}\n","      )\n","\n","predict_fn = load_predict_fn(saved_model_path)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YbGC8xefaYtV","colab_type":"text"},"source":["## Predict\n","\n","We can now call the predict method with different inputs each time and relatively quickly get results."]},{"cell_type":"code","metadata":{"id":"P9QDbIitQArX","colab_type":"code","colab":{}},"source":["def infer(text):\n","  return predict_fn([text])[0].decode('utf-8')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3WA0BYI9abgv","colab_type":"code","colab":{}},"source":["%%time\n","\n","for orig_text in [\"correction: \" + q for q in orig_texts]:\n","    print(infer(orig_text))"],"execution_count":0,"outputs":[]}]}