{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `heptabot` inference time and RAM usage\n",
    "\n",
    "In this notebook we will measure the performance of current `heptabot` version. This was tested on a [vast.ai](https://vast.ai/console/create/) instance created using `tensorflow/tensorflow:2.3.0-gpu-jupyter` image and our [Install](https://github.com/lcl-hse/heptabot/blob/gpu-tpu/notebooks/Install.ipynb) procedure. As `heptabot` is currently optimized for a NVidia GeForce GTX 1080 Ti-class graphics card with 16 GB total system RAM, the results will be shown for the same system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we check Python version and enter our working directory. Keep in mind that the code is executed within `heptabot` virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.9\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "FK6Icdp9uEBQ",
    "outputId": "1f3e267a-e044-4114-fa16-318fa7067bb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/heptabot\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the current load on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 18 00:50:01 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  On   | 00000000:03:00.0 Off |                  N/A |\n",
      "| 20%   34C    P8     7W / 220W |      1MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the total amount of used RAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:            15G        3.9G        7.4G        198M        4.2G         11G\n",
      "Swap:          7.6G        1.8G        5.8G\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the current CPU tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?1h\u001b=\u001b[H\u001b[2J\u001b[mtop - 00:50:43 up 57 days, 17:45,  2 users,  load average: 6.62, 6.63, 6.69\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "Threads:\u001b[m\u001b[m\u001b[1m  38 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m   1 \u001b[m\u001b[mrunning,\u001b[m\u001b[m\u001b[1m  37 \u001b[m\u001b[msleeping,\u001b[m\u001b[m\u001b[1m   0 \u001b[m\u001b[mstopped,\u001b[m\u001b[m\u001b[1m   0 \u001b[m\u001b[mzombie\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "%Cpu(s):\u001b[m\u001b[m\u001b[1m 42.5 \u001b[m\u001b[mus,\u001b[m\u001b[m\u001b[1m 32.0 \u001b[m\u001b[msy,\u001b[m\u001b[m\u001b[1m  1.0 \u001b[m\u001b[mni,\u001b[m\u001b[m\u001b[1m 24.0 \u001b[m\u001b[mid,\u001b[m\u001b[m\u001b[1m  0.3 \u001b[m\u001b[mwa,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mhi,\u001b[m\u001b[m\u001b[1m  0.1 \u001b[m\u001b[msi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mst\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "KiB Mem :\u001b[m\u001b[m\u001b[1m 16352884 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m  8173968 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m  3737368 \u001b[m\u001b[mused,\u001b[m\u001b[m\u001b[1m  4441548 \u001b[m\u001b[mbuff/cache\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "KiB Swap:\u001b[m\u001b[m\u001b[1m  8000508 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m  6121240 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m  1879268 \u001b[m\u001b[mused.\u001b[m\u001b[m\u001b[1m 12089668 \u001b[m\u001b[mavail Mem \u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "\u001b[K\n",
      "\u001b[7m  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND      \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m    1 root      20   0   20124   3200   3200 S  0.0  0.0   0:00.16 bash         \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m    8 root      20   0   20124   3240   3240 S  0.0  0.0   0:00.00 bash         \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m   20 root      20   0   72300   3200   3132 S  0.0  0.0   0:00.00 sshd         \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m   24 root      20   0   47660   5736   5392 S  0.0  0.0   0:00.64 ssh          \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m   88 root      20   0   27364   3880   3192 S  0.0  0.0   0:00.25 tmux: server \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m   89 root      20   0   20256   3388   3388 S  0.0  0.0   0:00.00 bash         \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  136 root      20   0  665908  82880  18172 S  0.0  0.5   0:12.68 jupyter-lab  \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  142 root      20   0  665908  82880  18172 S  0.0  0.5   0:00.00 jupyter-lab  \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  148 root      20   0  665908  82880  18172 S  0.0  0.5   0:00.00 jupyter-lab  \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  149 root      20   0  665908  82880  18172 S  0.0  0.5   0:00.04 jupyter-lab  \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  371 root      20   0  665908  82880  18172 S  0.0  0.5   0:00.00 jupyter-lab  \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  767 root      20   0  665908  82880  18172 S  0.0  0.5   0:00.00 jupyter-lab  \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  824 root      20   0  665908  82880  18172 S  0.0  0.5   0:00.00 jupyter-lab  \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  373 root      20   0  639628  50732  12936 S  0.0  0.3   0:00.58 python3.real \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  380 root      20   0  639628  50732  12936 S  0.0  0.3   0:00.00 python3.real \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  381 root      20   0  639628  50732  12936 S  0.0  0.3   0:00.01 python3.real \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  382 root      20   0  639628  50732  12936 S  0.0  0.3   0:00.02 python3.real \u001b[m\u001b[m\u001b[K\u001b[?1l\u001b>\u001b[25;1H\n",
      "\u001b[K"
     ]
    }
   ],
   "source": [
    "!top -H -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a way to place the models into CPU RAM: to do this, execute the code in the following cell. As we currently want to test the model on GPU, let's comment out this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ[\"MODEL_PLACE\"] = \"cpu\"\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's initialize the system. This will also download the missing `sentence_transformers` model in case it had not been done earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prompt_run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile prompt_run.sh\n",
    "source ~/mambaforge/etc/profile.d/conda.sh\n",
    "conda activate heptabot\n",
    "pyro4-ns &\n",
    "sleep 5; python models.py &\n",
    "sleep 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "i-65jplkvGVb",
    "outputId": "54ad523b-0f24-48c9-81a3-5dd9d0cf2be8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x7f7a20e633c8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "!chmod +x prompt_run.sh\n",
    "subprocess.Popen([\"/bin/bash\", os.path.join(os.path.abspath(os.getcwd()), \"prompt_run.sh\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the models are placed on GPU now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 18 01:50:37 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  On   | 00000000:03:00.0 Off |                  N/A |\n",
      "| 20%   34C    P8     8W / 220W |  10657MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is up and running thanks to `Pyro4`, but we still have to connect to it as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import Pyro4\n",
    "import Pyro4.util\n",
    "\n",
    "Heptamodel = Pyro4.Proxy(\"PYRONAME:heptabot.heptamodel\")\n",
    "batchify, process_batch, result_to_div = Heptamodel.batchify, Heptamodel.process_batch, Heptamodel.result_to_div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's unpack our example texts (each having 300 words, as measured by `nltk.word_tokenize`) and perform correction. You will find the correction results in `output` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir inputs\n",
    "!mkdir output\n",
    "!mv ./assets/example_texts.zip .\n",
    "!unzip -q example_texts.zip -d inputs\n",
    "\n",
    "texts = {}\n",
    "\n",
    "for f in os.listdir(\"inputs\"):\n",
    "  with open(os.path.join(\"inputs\", f), \"r\", encoding=\"utf-8\") as infile:\n",
    "    texts[f[:-4]] = infile.read()\n",
    "\n",
    "task_type = \"correction\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we actually perform the correction and benchmark the performance. The resulting time spent to process one document will be determined as the average over 5 text containing the same amount of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.7 ms, sys: 5.4 ms, total: 48.1 ms\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prepared_data = {}\n",
    "for textid in texts:\n",
    "    batches, delims = batchify(texts[textid], task_type)    \n",
    "    prepared_data[textid] = (batches, delims)\n",
    "\n",
    "with open(\"./templates/result.html\", \"r\") as inres:\n",
    "    outhtml = inres.read()\n",
    "outhtml = outhtml.replace(\"{{ which_font }}\", \"{0}\").replace(\"{{ response }}\", \"{1}\").replace(\"{{ task_type }}\", \"{2}\")\n",
    "\n",
    "processed_texts = {}\n",
    "which_font = \"\" if task_type == \"correction\" else \"font-family: Ubuntu Mono; letter-spacing: -0.5px;\"\n",
    "task_str = \"text\" if task_type == \"correction\" else \"sentences\"\n",
    "\n",
    "for textid in prepared_data:\n",
    "    batches, delims = prepared_data[textid]\n",
    "    processed = []\n",
    "\n",
    "    if task_type != \"correction\":\n",
    "        print(\"Processing text with ID\", textid)\n",
    "        for batch in tqdm(batches):\n",
    "            processed.append(process_batch(batch))\n",
    "    else:\n",
    "        for batch in batches:\n",
    "            processed.append(process_batch(batch))\n",
    "    plist = [item for subl in processed for item in subl] \n",
    "    response = result_to_div(texts[textid], plist, delims, task_type)\n",
    "    \n",
    "    proc_html = outhtml.format(which_font, response, task_str)\n",
    "    with open(os.path.join(\"output\", textid+\".html\"), \"w\", encoding=\"utf-8\") as outfile:\n",
    "        outfile.write(proc_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if something has changed on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 18 01:55:56 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  On   | 00000000:03:00.0 Off |                  N/A |\n",
      "| 40%   70C    P2    79W / 220W |  11001MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check our RAM and running processes again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:            15G        9.5G        3.4G        234M        2.7G        5.5G\n",
      "Swap:          7.6G        2.0G        5.7G\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?1h\u001b=\u001b[H\u001b[2J\u001b[mtop - 01:55:57 up 57 days, 18:50,  2 users,  load average: 8.41, 7.08, 6.86\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "Threads:\u001b[m\u001b[m\u001b[1m 114 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m   1 \u001b[m\u001b[mrunning,\u001b[m\u001b[m\u001b[1m 113 \u001b[m\u001b[msleeping,\u001b[m\u001b[m\u001b[1m   0 \u001b[m\u001b[mstopped,\u001b[m\u001b[m\u001b[1m   0 \u001b[m\u001b[mzombie\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "%Cpu(s):\u001b[m\u001b[m\u001b[1m 42.5 \u001b[m\u001b[mus,\u001b[m\u001b[m\u001b[1m 32.0 \u001b[m\u001b[msy,\u001b[m\u001b[m\u001b[1m  1.0 \u001b[m\u001b[mni,\u001b[m\u001b[m\u001b[1m 24.0 \u001b[m\u001b[mid,\u001b[m\u001b[m\u001b[1m  0.3 \u001b[m\u001b[mwa,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mhi,\u001b[m\u001b[m\u001b[1m  0.1 \u001b[m\u001b[msi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mst\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "KiB Mem :\u001b[m\u001b[m\u001b[1m 16352884 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m  3537236 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m 10002692 \u001b[m\u001b[mused,\u001b[m\u001b[m\u001b[1m  2812956 \u001b[m\u001b[mbuff/cache\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "KiB Swap:\u001b[m\u001b[m\u001b[1m  8000508 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m  5943832 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m  2056676 \u001b[m\u001b[mused.\u001b[m\u001b[m\u001b[1m  5787368 \u001b[m\u001b[mavail Mem \u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "\u001b[K\n",
      "\u001b[7m  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND      \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m\u001b[1m 1785 root      20   0   34516   3592   3168 R  6.2  0.0   0:00.01 top          \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m    1 root      20   0   20124   3200   3200 S  0.0  0.0   0:00.16 bash         \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m    8 root      20   0   20124   3240   3240 S  0.0  0.0   0:00.00 bash         \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m   20 root      20   0   72300   3244   3132 S  0.0  0.0   0:00.00 sshd         \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m   24 root      20   0   47660   5656   5392 S  0.0  0.0   0:00.91 ssh          \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 1299 root      20   0   27500   3764   3092 S  0.0  0.0   0:00.09 tmux: server \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 1300 root      20   0   20388   3548   3236 S  0.0  0.0   0:00.03 bash         \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 1326 root      20   0  507300  36908  13836 S  0.0  0.2   0:05.19 python3.real \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 1333 root      20   0  507300  36908  13836 S  0.0  0.2   0:00.00 python3.real \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 1336 root      20   0  507300  36908  13836 S  0.0  0.2   0:00.00 python3.real \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 1342 root      20   0  507300  36908  13836 S  0.0  0.2   0:00.00 python3.real \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 1343 root      20   0  507300  36908  13836 S  0.0  0.2   0:00.03 python3.real \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 1344 root      20   0  533744  13360   7584 S  0.0  0.1   0:00.46 python3.real \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 1364 root      20   0  533744  13360   7584 S  0.0  0.1   0:00.00 python3.real \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 1365 root      20   0  533744  13360   7584 S  0.0  0.1   0:00.00 python3.real \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 1366 root      20   0  533744  13360   7584 S  0.0  0.1   0:00.00 python3.real \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m 1367 root      20   0  533744  13360   7584 S  0.0  0.1   0:00.00 python3.real \u001b[m\u001b[m\u001b[K\u001b[?1l\u001b>\u001b[25;1H\n",
      "\u001b[K"
     ]
    }
   ],
   "source": [
    "!top -H -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it â€“ our benchmark ends here!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Processing texts with heptabot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "heptabot",
   "language": "python",
   "name": "heptabot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
